"""
Semantic Code Search Tool
Uses free sentence transformers for intelligent code search by meaning
"""

import os
import pickle
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np

try:
    from sentence_transformers import SentenceTransformer
    import faiss
    DEPENDENCIES_AVAILABLE = True
except ImportError:
    DEPENDENCIES_AVAILABLE = False

from .base import BaseTool


@dataclass
class CodeChunk:
    """Represents a piece of code with metadata"""
    file_path: str
    start_line: int
    end_line: int
    content: str
    function_name: Optional[str] = None
    class_name: Optional[str] = None
    chunk_type: str = "code"  # code, function, class, comment


class SemanticSearchTool(BaseTool):
    """Tool for semantic search through codebase"""
    
    def __init__(self, workspace_path: str):
        super().__init__("semantic_search", "Ð¡ÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð¿Ð¾ ÐºÐ¾Ð´Ð¾Ð²Ð¾Ð¹ Ð±Ð°Ð·Ðµ")
        self.workspace_path = Path(workspace_path)
        self.cache_dir = self.workspace_path / ".ai-punk-cache"
        self.cache_dir.mkdir(exist_ok=True)
        
        # Files to index
        self.code_extensions = {'.py', '.js', '.ts', '.java', '.cpp', '.c', '.h', '.cs', '.go', '.rs', '.php', '.rb'}
        self.doc_extensions = {'.md', '.txt', '.rst', '.org'}
        
        # Model and index
        self.model = None
        self.index = None
        self.chunks = []
        self.is_initialized = False
        
    def _check_dependencies(self):
        """Check if required dependencies are available"""
        if not DEPENDENCIES_AVAILABLE:
            raise ImportError(
                "Ð”Ð»Ñ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ° Ð½ÑƒÐ¶Ð½Ð¾ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ:\n"
                "pip install sentence-transformers faiss-cpu"
            )
    
    def _initialize_model(self):
        """Initialize the embedding model"""
        if self.model is None:
            self._check_dependencies()
            print("ðŸ¤– Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°...")
            # Use free, lightweight model optimized for code
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
            print("âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°!")
    
    def _get_cache_key(self) -> str:
        """Generate cache key based on codebase content"""
        # Get modification times of all relevant files
        file_times = []
        for ext in self.code_extensions | self.doc_extensions:
            for file_path in self.workspace_path.rglob(f"*{ext}"):
                if self._should_index_file(file_path):
                    file_times.append(f"{file_path}:{file_path.stat().st_mtime}")
        
        # Create hash from all file modification times
        content = "\n".join(sorted(file_times))
        return hashlib.md5(content.encode()).hexdigest()
    
    def _should_index_file(self, file_path: Path) -> bool:
        """Check if file should be indexed"""
        # Skip hidden files and directories
        if any(part.startswith('.') for part in file_path.parts):
            # But allow .ai-punk-cache
            if '.ai-punk-cache' not in str(file_path):
                return False
        
        # Skip large files (>1MB)
        try:
            if file_path.stat().st_size > 1024 * 1024:
                return False
        except:
            return False
            
        # Skip binary files
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                f.read(100)  # Try to read first 100 chars
        except:
            return False
            
        return True
    
    def _extract_code_chunks(self, file_path: Path) -> List[CodeChunk]:
        """Extract meaningful code chunks from a file"""
        chunks = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except:
            return chunks
        
        rel_path = str(file_path.relative_to(self.workspace_path))
        
        # Simple chunking strategy
        current_chunk = []
        start_line = 1
        
        for i, line in enumerate(lines, 1):
            current_chunk.append(line.rstrip())
            
            # Create chunk on empty line or every 20 lines
            if (not line.strip() and current_chunk) or len(current_chunk) >= 20:
                if current_chunk and any(line.strip() for line in current_chunk):
                    content = '\n'.join(current_chunk)
                    chunks.append(CodeChunk(
                        file_path=rel_path,
                        start_line=start_line,
                        end_line=i,
                        content=content,
                        chunk_type="code"
                    ))
                current_chunk = []
                start_line = i + 1
        
        # Add remaining chunk
        if current_chunk and any(line.strip() for line in current_chunk):
            content = '\n'.join(current_chunk)
            chunks.append(CodeChunk(
                file_path=rel_path,
                start_line=start_line,
                end_line=len(lines),
                content=content,
                chunk_type="code"
            ))
        
        return chunks
    
    def _load_cache(self) -> bool:
        """Load index from cache if available"""
        cache_key = self._get_cache_key()
        cache_file = self.cache_dir / f"semantic_index_{cache_key}.pkl"
        faiss_file = self.cache_dir / f"semantic_index_{cache_key}.faiss"
        
        if cache_file.exists() and faiss_file.exists():
            try:
                print("ðŸ“¦ Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¸Ð½Ð´ÐµÐºÑ...")
                with open(cache_file, 'rb') as f:
                    self.chunks = pickle.load(f)
                
                self.index = faiss.read_index(str(faiss_file))
                print(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð¸Ð½Ð´ÐµÐºÑ Ñ {len(self.chunks)} Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸ ÐºÐ¾Ð´Ð°")
                return True
            except Exception as e:
                print(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ÐºÑÑˆÐ°: {e}")
                
        return False
    
    def _save_cache(self):
        """Save index to cache"""
        cache_key = self._get_cache_key()
        cache_file = self.cache_dir / f"semantic_index_{cache_key}.pkl"
        faiss_file = self.cache_dir / f"semantic_index_{cache_key}.faiss"
        
        try:
            # Clean old cache files
            for old_file in self.cache_dir.glob("semantic_index_*.pkl"):
                old_file.unlink()
            for old_file in self.cache_dir.glob("semantic_index_*.faiss"):
                old_file.unlink()
            
            # Save new cache
            with open(cache_file, 'wb') as f:
                pickle.dump(self.chunks, f)
            
            faiss.write_index(self.index, str(faiss_file))
            print(f"ðŸ’¾ Ð˜Ð½Ð´ÐµÐºÑ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð² ÐºÑÑˆ")
        except Exception as e:
            print(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ ÐºÑÑˆÐ°: {e}")
    
    def index_codebase(self) -> Dict[str, Any]:
        """Index the entire codebase for semantic search"""
        try:
            self._initialize_model()
            
            # Try to load from cache first
            if self._load_cache():
                self.is_initialized = True
                return self._format_success(f"Ð˜Ð½Ð´ÐµÐºÑ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð¸Ð· ÐºÑÑˆÐ°: {len(self.chunks)} Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð²")
            
            print("ðŸ” Ð¡ÐºÐ°Ð½Ð¸Ñ€ÑƒÑŽ ÐºÐ¾Ð´Ð¾Ð²ÑƒÑŽ Ð±Ð°Ð·Ñƒ...")
            
            # Collect all code chunks
            all_chunks = []
            file_count = 0
            
            for ext in self.code_extensions | self.doc_extensions:
                for file_path in self.workspace_path.rglob(f"*{ext}"):
                    if self._should_index_file(file_path):
                        chunks = self._extract_code_chunks(file_path)
                        all_chunks.extend(chunks)
                        file_count += 1
                        
                        if file_count % 10 == 0:
                            print(f"  ðŸ“ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð²: {file_count}")
            
            if not all_chunks:
                return self._format_error("ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸")
            
            print(f"ðŸ“Š ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(all_chunks)} Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð² ÐºÐ¾Ð´Ð° Ð² {file_count} Ñ„Ð°Ð¹Ð»Ð°Ñ…")
            print("ðŸ§  Ð¡Ð¾Ð·Ð´Ð°ÑŽ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸...")
            
            # Create embeddings
            texts = [chunk.content for chunk in all_chunks]
            embeddings = self.model.encode(texts, show_progress_bar=True, batch_size=32)
            
            # Create FAISS index
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
            
            # Normalize embeddings for cosine similarity
            faiss.normalize_L2(embeddings)
            self.index.add(embeddings)
            
            self.chunks = all_chunks
            self.is_initialized = True
            
            # Save to cache
            self._save_cache()
            
            return self._format_success(
                f"ÐšÐ¾Ð´Ð¾Ð²Ð°Ñ Ð±Ð°Ð·Ð° Ð¿Ñ€Ð¾Ð¸Ð½Ð´ÐµÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð°: {len(all_chunks)} Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ð¸Ð· {file_count} Ñ„Ð°Ð¹Ð»Ð¾Ð²"
            )
            
        except Exception as e:
            return self._format_error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸: {str(e)}")
    
    def search(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """Search code semantically"""
        try:
            if not self.is_initialized:
                init_result = self.index_codebase()
                if not init_result["success"]:
                    return init_result
            
            # Create query embedding
            query_embedding = self.model.encode([query])
            faiss.normalize_L2(query_embedding)
            
            # Search
            scores, indices = self.index.search(query_embedding, limit)
            
            # Debug information
            print(f"ðŸ” Debug: Found {len(scores[0])} results, top scores: {scores[0][:3]}")
            
            # Format results
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx >= 0:  # Ð£Ð±Ð¸Ñ€Ð°ÑŽ Ð¿Ð¾Ñ€Ð¾Ð³ ÑÑ…Ð¾Ð¶ÐµÑÑ‚Ð¸ Ð´Ð»Ñ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
                    chunk = self.chunks[idx]
                    results.append({
                        "file": chunk.file_path,
                        "lines": f"{chunk.start_line}-{chunk.end_line}",
                        "content": chunk.content[:200] + "..." if len(chunk.content) > 200 else chunk.content,
                        "score": float(score),
                        "type": chunk.chunk_type
                    })
            
            # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ð° Ð²ÐµÑ€Ñ…Ð½ÐµÐ¼ ÑƒÑ€Ð¾Ð²Ð½Ðµ Ð´Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸
            result = self._format_success(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(results)} Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð²")
            result["query"] = query
            result["results"] = results
            result["total_found"] = len(results)
            return result
            
        except Exception as e:
            return self._format_error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð¸ÑÐºÐ°: {str(e)}")
    
    def execute(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """Execute semantic search"""
        return self.search(query, limit)


def create_semantic_search_tool(workspace_path: str) -> SemanticSearchTool:
    """Factory function to create semantic search tool"""
    return SemanticSearchTool(workspace_path) 